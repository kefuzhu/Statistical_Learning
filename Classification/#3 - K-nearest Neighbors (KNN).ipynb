{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bf9e20c",
   "metadata": {},
   "source": [
    "# K-nearest Neighbors (KNN)\n",
    "\n",
    "KNN is a non-parametric supervised learning which assigns class label to new data point based on the class labels of $k$ closest known data points in multidimensional feature space. This algorithm relies on distance for classification\n",
    "\n",
    "## Distance Metrics\n",
    "\n",
    "**Minkowski Distance**: It is a metric intended for real-valued vector space. The feature space should not have negative distance\n",
    "\n",
    "<center>\n",
    "$d = (\\sum_{i=1}^{n} |x_i - y_i|^p)^{1/p}$\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "**Manhattan Distance**: The distance between two points is the sum of the absolute differences of their Cartesian coordinates. Specification from Minkowski Distance with $p = 1$\n",
    "\n",
    "<center>\n",
    "$d = \\sum_{i=1}^{n} |x_i - y_i|$\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "**Euclidean Distance**: The distance between two points is the true straight line distance in Cartesian space. Specification from Minkowski Distance with $p = 2$\n",
    "\n",
    "<center>\n",
    "$d = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}$\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "**Cosine Distance**: The distance between two points is the cosine of the angle between two vectors and determines whether two vectors are pointing in the same direction. This distance metric is used mainly to calculate similarity between two vectors and is commonly used in text analysis to find similarities between two documents by the number of times a particular set of words appear in it.\n",
    "\n",
    "<center>\n",
    "$d = \\cos \\theta = \\frac{\\vec{a} \\cdot \\vec{b}}{||\\vec{a}|| \\cdot ||\\vec{b}||} = \\frac{\\sum_{i=1}^{n} a_i b_i}{\\sqrt{\\sum_{i=1}^{n} a_i^2} \\sqrt{\\sum_{i=1}^{n} b_i^2}}$\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "**Jaccard Distance**: Jaccard index, also known as Jaccard similarity coefficient, measures the similarity and diversity of sample sets. The Jaccard distance is the complement of the Jaccard index and it measures the dissimilarity between sample sets.\n",
    "\n",
    "<center>\n",
    "$d = 1 - J(A,B) = 1 - \\frac{|A \\cap B|}{|A \\cup B|} = 1 - \\frac{|A \\cap B|}{|A| + |B| - |A \\cap B|} = \\frac{|A \\cup B| - |A \\cap B|}{|A \\cup B|}$\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "**Hamming Distance**: Hamming distance between two strings of equal length is the number of positions at which the corresponding symbols are different. In other words, it measures the minimum number of substitutions required to change one string into the other, or the minimum number of errors that could have transformed one string into the other. In a more general context, the Hamming distance is one of several string metrics for measuring the edit distance between two sequences.\n",
    "\n",
    "## Assumptions\n",
    "\n",
    "1. **Independence of observations**: There is no relationship between each observation/data point. One opposite example is time-series data\n",
    "2. **Linear relationship**: the change in the response due to a one-unit change in a predictor is constant, regardless of the value of that predictor\n",
    "3. **Normality of residuals**: residuals should follow normal distribution, which can be assessed by histogram or Q-Q plot\n",
    "4. **No or little multicollinearity**: predictors should not be highly correlated with each other, which can be assessed by VIF (Variance Inflation Factor)\n",
    "5. **Homoscedasticity or constant variance**: the error is constant along the values of the predictor\n",
    "\n",
    "\n",
    "## Advantages\n",
    "\n",
    "1. Support vector classifier's decision rule (i.e. hyperplane) is only affected by a potentially small subset of the traning observations (i.e. support vectors). Therefore, **SVM is quite robust to the behavior of other observations that are far away from the hyperplane (i.e. extreme values)**\n",
    "\n",
    "## Limitations\n",
    "\n",
    "1. Hyperplane used in SVM can only separate two classes (i.e. binary classification). Workarounds including *one-versus-one* and *one-versus all* can be used in addressing multiclass classification. However, it may not be effective.\n",
    "\n",
    "\n",
    "RESUME READING ON 9.3.1, page 363"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e62aa08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85baff9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
